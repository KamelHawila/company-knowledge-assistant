{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9978729f",
   "metadata": {},
   "source": [
    "# 01 — LLM Fundamentals\n",
    "\n",
    "This notebook builds intuition for how language models work by implementing\n",
    "tokenization, padding, softmax, and sampling manually before using frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec40700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD = \"[PAD]\"\n",
    "UNK = \"[UNK]\"\n",
    "BOS = \"[BOS]\"\n",
    "EOS = \"[EOS]\"\n",
    "\n",
    "# Toy vocabulary\n",
    "vocab = {\n",
    "    PAD: 0,\n",
    "    UNK: 1,\n",
    "    BOS: 2,\n",
    "    EOS: 3,\n",
    "    \"i\": 4,\n",
    "    \"love\": 5,\n",
    "    \"machine\": 6,\n",
    "    \"learning\": 7,\n",
    "    \"nlp\": 8,\n",
    "    \"is\": 9,\n",
    "    \"fun\": 10,\n",
    "}\n",
    "\n",
    "id_to_token = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4cd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    tokens = text.lower().split()\n",
    "    ids = [vocab.get(token, vocab[UNK]) for token in tokens]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46de3fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 1, 7]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"i love deep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7872d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    tokens = [id_to_token[i] for i in ids]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154c1015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love [UNK] learning'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([4, 5, 1, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a039c902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 5, 8, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_with_special_tokens(text: str):\n",
    "    return [vocab[BOS]] + encode(text) + [vocab[EOS]]\n",
    "\n",
    "encode_with_special_tokens(\"i love nlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e57daa",
   "metadata": {},
   "source": [
    "- **[UNK]** represents a token that is not present in the tokenizer vocabulary.\n",
    "- **[PAD]** is used to make sequences the same length so they can be processed together in batches; padded positions are ignored using an attention mask.\n",
    "- **[BOS]** marks the beginning of a sequence and **[EOS]** marks the end, helping the model understand sequence boundaries during training and generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1883cf",
   "metadata": {},
   "source": [
    "## Task 2 — Padding and Attention Masks\n",
    "Transformers process batches of sequences with equal length using padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3154b76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 5, 8, 3], [2, 4, 5, 1, 7, 3], [2, 4, 5, 6, 7, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example sentences with different lengths\n",
    "sentences = [\n",
    "    \"i love nlp\",\n",
    "    \"i love deep learning\",\n",
    "    \"i love machine learning\",\n",
    "]\n",
    "\n",
    "encoded = [encode_with_special_tokens(s) for s in sentences]\n",
    "encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9bdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[2, 4, 5, 8, 3, 0], [2, 4, 5, 1, 7, 3], [2, 4, 5, 6, 7, 3]], 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sequences(batch_ids, pad_id: int):\n",
    "    max_len = max(len(x) for x in batch_ids)\n",
    "    padded = []\n",
    "    for x in batch_ids:\n",
    "        padded.append(x + [pad_id] * (max_len - len(x)))\n",
    "    return padded, max_len\n",
    "\n",
    "pad_id = vocab[PAD]\n",
    "\n",
    "padded_ids, max_len = pad_sequences(encoded, pad_id=pad_id)\n",
    "\n",
    "padded_ids, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832c0955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_attention_mask(padded_batch_ids, pad_id: int):\n",
    "    return [[1 if i != pad_id else 0 for i in x] for x in padded_batch_ids]\n",
    "\n",
    "attention_mask = make_attention_mask(padded_ids, pad_id=pad_id)\n",
    "\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec3dd969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "tokens:  ['[BOS]', 'i', 'love', 'nlp', '[EOS]', '[PAD]']\n",
      "mask:    [1, 1, 1, 1, 1, 0]\n",
      "\n",
      "Example 1\n",
      "tokens:  ['[BOS]', 'i', 'love', '[UNK]', 'learning', '[EOS]']\n",
      "mask:    [1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 2\n",
      "tokens:  ['[BOS]', 'i', 'love', 'machine', 'learning', '[EOS]']\n",
      "mask:    [1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_batch(padded_batch_ids, attention_mask):\n",
    "    for i, (seq, mask) in enumerate(zip(padded_batch_ids, attention_mask)):\n",
    "        tokens = [id_to_token[t] for t in seq]\n",
    "        print(f\"Example {i}\")\n",
    "        print(\"tokens: \", tokens)\n",
    "        print(\"mask:   \", mask)\n",
    "        print()\n",
    "\n",
    "pretty_print_batch(padded_ids, attention_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597732a7",
   "metadata": {},
   "source": [
    "1) We pad sequences so they all have the same length, allowing them to be processed together in a batch by the model.\n",
    "\n",
    "2) If we do not use an attention mask, the model will treat padding tokens as real input tokens, which can distort attention scores and negatively affect the model’s understanding of the actual sentence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
